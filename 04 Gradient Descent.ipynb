{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc02420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2fe23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22c2da",
   "metadata": {},
   "source": [
    "# everything manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c657ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: 0.000\n",
      "epoch: 1 loss: 30.00000000\n",
      "epoch: 2 loss: 4.80000067\n",
      "epoch: 3 loss: 0.76800019\n",
      "epoch: 4 loss: 0.12288000\n",
      "epoch: 5 loss: 0.01966083\n",
      "epoch: 6 loss: 0.00314574\n",
      "epoch: 7 loss: 0.00050332\n",
      "epoch: 8 loss: 0.00008053\n",
      "epoch: 9 loss: 0.00001288\n",
      "epoch: 10 loss: 0.00000206\n",
      "prediction after training: 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "W = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_hat):\n",
    "    return ((y - y_hat)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x -y)**2\n",
    "# dJ/dw = 1/N 2x (w*x - y)\n",
    "def gradient(x, y, y_hat):\n",
    "    return np.dot(2*x, y_hat - y).mean()\n",
    "\n",
    "print(f'prediction before training: {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "lr = 0.01\n",
    "n_iters = 10\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(Y, y_hat)\n",
    "\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, y_hat)\n",
    "\n",
    "    W -= lr * dw\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} loss: {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b004d71",
   "metadata": {},
   "source": [
    "# replace manual gradient computation using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3226342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: 0.000\n",
      "epoch: 1 loss: 30.00000000\n",
      "epoch: 2 loss: 21.67499924\n",
      "epoch: 3 loss: 15.66018772\n",
      "epoch: 4 loss: 11.31448650\n",
      "epoch: 5 loss: 8.17471695\n",
      "epoch: 6 loss: 5.90623236\n",
      "epoch: 7 loss: 4.26725292\n",
      "epoch: 8 loss: 3.08308983\n",
      "epoch: 9 loss: 2.22753215\n",
      "epoch: 10 loss: 1.60939169\n",
      "epoch: 11 loss: 1.16278565\n",
      "epoch: 12 loss: 0.84011245\n",
      "epoch: 13 loss: 0.60698116\n",
      "epoch: 14 loss: 0.43854395\n",
      "epoch: 15 loss: 0.31684780\n",
      "epoch: 16 loss: 0.22892261\n",
      "epoch: 17 loss: 0.16539653\n",
      "epoch: 18 loss: 0.11949898\n",
      "epoch: 19 loss: 0.08633806\n",
      "epoch: 20 loss: 0.06237914\n",
      "prediction after training: 9.612\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_hat):\n",
    "    return ((y - y_hat)**2).mean()\n",
    "\n",
    "print(f'prediction before training: {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "lr = 0.01\n",
    "n_iters = 20\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(Y, y_hat) # loss computed\n",
    "\n",
    "    # compute gradient of current tensor wrt graph leaves\n",
    "    l.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W -= W.grad * lr\n",
    "\n",
    "    W.grad.zero_()\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} loss: {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472ae81",
   "metadata": {},
   "source": [
    "# replace manual loss calculation and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd827af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: 0.000\n",
      "epoch: 1 loss: 30.00000000\n",
      "epoch: 2 loss: 21.67499924\n",
      "epoch: 3 loss: 15.66018772\n",
      "epoch: 4 loss: 11.31448650\n",
      "epoch: 5 loss: 8.17471695\n",
      "epoch: 6 loss: 5.90623236\n",
      "epoch: 7 loss: 4.26725292\n",
      "epoch: 8 loss: 3.08308983\n",
      "epoch: 9 loss: 2.22753215\n",
      "epoch: 10 loss: 1.60939169\n",
      "epoch: 11 loss: 1.16278565\n",
      "epoch: 12 loss: 0.84011245\n",
      "epoch: 13 loss: 0.60698116\n",
      "epoch: 14 loss: 0.43854395\n",
      "epoch: 15 loss: 0.31684780\n",
      "epoch: 16 loss: 0.22892261\n",
      "epoch: 17 loss: 0.16539653\n",
      "epoch: 18 loss: 0.11949898\n",
      "epoch: 19 loss: 0.08633806\n",
      "epoch: 20 loss: 0.06237914\n",
      "prediction after training: 9.612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "# loss = MSE\n",
    "loss = nn.MSELoss()\n",
    "optim = torch.optim.SGD([W], lr=0.01)\n",
    "\n",
    "print(f'prediction before training: {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "n_iters = 20\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(Y, y_hat) # loss computed\n",
    "\n",
    "    # compute gradient of current tensor wrt graph leaves\n",
    "    l.backward()\n",
    "\n",
    "    optim.step() # update weights\n",
    "\n",
    "    optim.zero_grad() # reset gradient accumulation to zero\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} loss: {l:.8f}')\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec186a5d",
   "metadata": {},
   "source": [
    "# replace forward method with pytorch model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6cc704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: -2.798\n",
      "epoch: 1 w: -0.34948939085006714 loss: 45.28266525 \n",
      "epoch: 2 w: -0.0425642728805542 loss: 31.53902054 \n",
      "epoch: 3 w: 0.2133583426475525 loss: 22.00188828 \n",
      "epoch: 4 w: 0.4267953634262085 loss: 15.38357258 \n",
      "epoch: 5 w: 0.604841411113739 loss: 10.79056358 \n",
      "epoch: 6 w: 0.753406286239624 loss: 7.60287619 \n",
      "epoch: 7 w: 0.8774126768112183 loss: 5.39031696 \n",
      "epoch: 8 w: 0.980961263179779 loss: 3.85438204 \n",
      "epoch: 9 w: 1.0674679279327393 loss: 2.78794670 \n",
      "epoch: 10 w: 1.1397778987884521 loss: 2.04729176 \n",
      "epoch: 11 w: 1.2002614736557007 loss: 1.53269255 \n",
      "epoch: 12 w: 1.250892996788025 loss: 1.17495275 \n",
      "epoch: 13 w: 1.2933169603347778 loss: 0.92605811 \n",
      "epoch: 14 w: 1.3289034366607666 loss: 0.75269306 \n",
      "epoch: 15 w: 1.3587934970855713 loss: 0.63174039 \n",
      "epoch: 16 w: 1.3839378356933594 loss: 0.54715943 \n",
      "epoch: 17 w: 1.4051282405853271 loss: 0.48782015 \n",
      "epoch: 18 w: 1.423024296760559 loss: 0.44599921 \n",
      "epoch: 19 w: 1.4381754398345947 loss: 0.41633755 \n",
      "epoch: 20 w: 1.451039433479309 loss: 0.39511704 \n",
      "prediction after training: 8.683\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5.0], dtype=torch.float)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "model = nn.Linear(n_features, n_features)\n",
    "\n",
    "# loss = MSE\n",
    "loss = nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f'prediction before training: {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "n_iters = 20\n",
    "for i in range(n_iters):\n",
    "    y_hat = model(X)\n",
    "    l = loss(Y, y_hat) # loss computed\n",
    "\n",
    "    # compute gradient of current tensor wrt graph leaves\n",
    "    l.backward()\n",
    "\n",
    "    optim.step() # update weights\n",
    "\n",
    "    optim.zero_grad() # reset gradient accumulation to zero\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        w, b = model.parameters()\n",
    "        print(f'epoch: {i+1} w: {w.item()} loss: {l:.8f} ')\n",
    "\n",
    "print(f'prediction after training: {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06bb0d",
   "metadata": {},
   "source": [
    "# practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b380c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model training: 0.0000\n",
      "epoch: 1 W: 1.2000 loss:30.0000 forward: 5.999999523162842\n",
      "epoch: 2 W: 1.6800 loss:4.8000 forward: 8.399999618530273\n",
      "epoch: 3 W: 1.8720 loss:0.7680 forward: 9.359999656677246\n",
      "epoch: 4 W: 1.9488 loss:0.1229 forward: 9.743999481201172\n",
      "epoch: 5 W: 1.9795 loss:0.0197 forward: 9.897600173950195\n",
      "epoch: 6 W: 1.9918 loss:0.0031 forward: 9.959039688110352\n",
      "epoch: 7 W: 1.9967 loss:0.0005 forward: 9.98361587524414\n",
      "epoch: 8 W: 1.9987 loss:0.0001 forward: 9.993446350097656\n",
      "epoch: 9 W: 1.9995 loss:0.0000 forward: 9.9973783493042\n",
      "epoch: 10 W: 1.9998 loss:0.0000 forward: 9.99895191192627\n",
      "after model training: 9.9990\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "W = 0.0\n",
    "\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "def loss(y_hat, y):\n",
    "    return ((y_hat - y) ** 2).mean()\n",
    "\n",
    "def gradient(X, Y, y_hat):\n",
    "    return np.dot(2 * X, y_hat - Y).mean()\n",
    "\n",
    "\n",
    "n_iters = 10\n",
    "lr = 0.01\n",
    "\n",
    "print(f'before model training: {forward(5):.4f}')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(y_hat, Y)\n",
    "\n",
    "    dw = gradient(X, Y, y_hat)\n",
    "    W -= lr * dw\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} W: {W:.4f} loss:{l:.4f} forward: {forward(5)}')\n",
    "\n",
    "print(f'after model training: {forward(5):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e1c94aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training: -4.255657196044922\n",
      "epoch: 1 loss: 66.1850\n",
      "epoch: 2 loss: 45.9244\n",
      "epoch: 3 loss: 31.8660\n",
      "epoch: 4 loss: 22.1112\n",
      "epoch: 5 loss: 15.3426\n",
      "epoch: 6 loss: 10.6459\n",
      "epoch: 7 loss: 7.3870\n",
      "epoch: 8 loss: 5.1258\n",
      "epoch: 9 loss: 3.5567\n",
      "epoch: 10 loss: 2.4680\n",
      "epoch: 11 loss: 1.7125\n",
      "epoch: 12 loss: 1.1883\n",
      "epoch: 13 loss: 0.8246\n",
      "epoch: 14 loss: 0.5722\n",
      "epoch: 15 loss: 0.3971\n",
      "epoch: 16 loss: 0.2756\n",
      "epoch: 17 loss: 0.1913\n",
      "epoch: 18 loss: 0.1328\n",
      "epoch: 19 loss: 0.0922\n",
      "epoch: 20 loss: 0.0640\n",
      "epoch: 21 loss: 0.0445\n",
      "epoch: 22 loss: 0.0309\n",
      "epoch: 23 loss: 0.0215\n",
      "epoch: 24 loss: 0.0149\n",
      "epoch: 25 loss: 0.0104\n",
      "epoch: 26 loss: 0.0073\n",
      "epoch: 27 loss: 0.0051\n",
      "epoch: 28 loss: 0.0036\n",
      "epoch: 29 loss: 0.0025\n",
      "epoch: 30 loss: 0.0018\n",
      "after training: 9.920428276062012\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "X_test = torch.tensor([5.0], dtype=torch.float)\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float)\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "    \n",
    "n_samples, n_features = X.shape\n",
    "model = LinearModel(n_features, n_features)\n",
    "\n",
    "print(f'before training: {model(X_test).item()}')\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "n_iters = 30\n",
    "for epoch in range(n_iters):\n",
    "    y_hat = model(X)\n",
    "    l = loss(y_hat, Y)\n",
    "\n",
    "    l.backward() # compute gradients\n",
    "    \n",
    "    optim.step() # update the weights\n",
    "\n",
    "    optim.zero_grad() # reset the gradient accumulation\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch: {epoch + 1} loss: {l:.4f}')\n",
    "\n",
    "\n",
    "print(f'after training: {model(X_test).item()}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ec82fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model training: 0.0000\n",
      "epoch: 1 W: 0.3000 loss:30.0000 forward: 1.4999998807907104\n",
      "epoch: 2 W: 0.5550 loss:21.6750 forward: 2.7749996185302734\n",
      "epoch: 3 W: 0.7717 loss:15.6602 forward: 3.8587496280670166\n",
      "epoch: 4 W: 0.9560 loss:11.3145 forward: 4.779937267303467\n",
      "epoch: 5 W: 1.1126 loss:8.1747 forward: 5.562946796417236\n",
      "epoch: 6 W: 1.2457 loss:5.9062 forward: 6.228504657745361\n",
      "epoch: 7 W: 1.3588 loss:4.2673 forward: 6.794229030609131\n",
      "epoch: 8 W: 1.4550 loss:3.0831 forward: 7.275094985961914\n",
      "epoch: 9 W: 1.5368 loss:2.2275 forward: 7.683830738067627\n",
      "epoch: 10 W: 1.6063 loss:1.6094 forward: 8.031255722045898\n",
      "epoch: 11 W: 1.6653 loss:1.1628 forward: 8.326567649841309\n",
      "epoch: 12 W: 1.7155 loss:0.8401 forward: 8.577583312988281\n",
      "epoch: 13 W: 1.7582 loss:0.6070 forward: 8.790945053100586\n",
      "epoch: 14 W: 1.7945 loss:0.4385 forward: 8.97230339050293\n",
      "epoch: 15 W: 1.8253 loss:0.3168 forward: 9.126458168029785\n",
      "epoch: 16 W: 1.8515 loss:0.2289 forward: 9.257489204406738\n",
      "epoch: 17 W: 1.8738 loss:0.1654 forward: 9.368865966796875\n",
      "epoch: 18 W: 1.8927 loss:0.1195 forward: 9.463536262512207\n",
      "epoch: 19 W: 1.9088 loss:0.0863 forward: 9.54400634765625\n",
      "epoch: 20 W: 1.9225 loss:0.0624 forward: 9.612405776977539\n",
      "after model training: 9.6124\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "def loss(y_hat, y):\n",
    "    return ((y_hat - y) ** 2).mean()\n",
    "\n",
    "n_iters = 20\n",
    "lr = 0.01\n",
    "\n",
    "print(f'before model training: {forward(5):.4f}')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(y_hat, Y)\n",
    "\n",
    "    # dw = gradient(X, Y, y_hat)\n",
    "    l.backward()\n",
    "    dw = W.grad\n",
    "    with torch.no_grad():\n",
    "        W -= lr * dw\n",
    "\n",
    "    W.grad.zero_() # reset gradient accumulation\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} W: {W:.4f} loss:{l:.4f} forward: {forward(5)}')\n",
    "\n",
    "print(f'after model training: {forward(5):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f98659d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model training: 0.0000\n",
      "epoch: 1 W: 0.3000 loss:30.0000 forward: 1.4999998807907104\n",
      "epoch: 2 W: 0.5550 loss:21.6750 forward: 2.7749996185302734\n",
      "epoch: 3 W: 0.7717 loss:15.6602 forward: 3.8587496280670166\n",
      "epoch: 4 W: 0.9560 loss:11.3145 forward: 4.779937267303467\n",
      "epoch: 5 W: 1.1126 loss:8.1747 forward: 5.562946796417236\n",
      "epoch: 6 W: 1.2457 loss:5.9062 forward: 6.228504657745361\n",
      "epoch: 7 W: 1.3588 loss:4.2673 forward: 6.794229030609131\n",
      "epoch: 8 W: 1.4550 loss:3.0831 forward: 7.275094985961914\n",
      "epoch: 9 W: 1.5368 loss:2.2275 forward: 7.683830738067627\n",
      "epoch: 10 W: 1.6063 loss:1.6094 forward: 8.031255722045898\n",
      "epoch: 11 W: 1.6653 loss:1.1628 forward: 8.326567649841309\n",
      "epoch: 12 W: 1.7155 loss:0.8401 forward: 8.577583312988281\n",
      "epoch: 13 W: 1.7582 loss:0.6070 forward: 8.790945053100586\n",
      "epoch: 14 W: 1.7945 loss:0.4385 forward: 8.97230339050293\n",
      "epoch: 15 W: 1.8253 loss:0.3168 forward: 9.126458168029785\n",
      "epoch: 16 W: 1.8515 loss:0.2289 forward: 9.257489204406738\n",
      "epoch: 17 W: 1.8738 loss:0.1654 forward: 9.368865966796875\n",
      "epoch: 18 W: 1.8927 loss:0.1195 forward: 9.463536262512207\n",
      "epoch: 19 W: 1.9088 loss:0.0863 forward: 9.54400634765625\n",
      "epoch: 20 W: 1.9225 loss:0.0624 forward: 9.612405776977539\n",
      "after model training: 9.6124\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return W * x\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optim = torch.optim.SGD([W], lr=lr)\n",
    "\n",
    "n_iters = 20\n",
    "\n",
    "print(f'before model training: {forward(5):.4f}')\n",
    "\n",
    "for i in range(n_iters):\n",
    "    y_hat = forward(X)\n",
    "    l = loss(y_hat, Y)\n",
    "\n",
    "    l.backward() # compute gradient\n",
    "    optim.step() # update weights\n",
    "    optim.zero_grad() # reset accumulated gradients\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'epoch: {i+1} W: {W:.4f} loss:{l:.4f} forward: {forward(5)}')\n",
    "\n",
    "print(f'after model training: {forward(5):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4423c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
